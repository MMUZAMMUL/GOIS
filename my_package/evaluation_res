from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def evaluate_predictions(ground_truth_path, predictions_path, iou_type='bbox'):
    """
    Evaluate predictions using COCO metrics.
    """
    print(f"Evaluating predictions...\nGround Truth: {ground_truth_path}\nPredictions: {predictions_path}")
    coco_gt = COCO(ground_truth_path)
    coco_dt = coco_gt.loadRes(predictions_path)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType=iou_type)
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

def get_evaluation_metrics(ground_truth_path, predictions_path, iou_type='bbox'):
    """
    Get evaluation metrics as a dictionary.
    """
    coco_gt = COCO(ground_truth_path)
    coco_dt = coco_gt.loadRes(predictions_path)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType=iou_type)
    coco_eval.evaluate()
    coco_eval.accumulate()
    metrics = {
        "AP@[IoU=0.50:0.95]": coco_eval.stats[0],
        "AP@[IoU=0.50]": coco_eval.stats[1],
        "AP@[IoU=0.75]": coco_eval.stats[2],
        "AP@[small]": coco_eval.stats[3],
        "AP@[medium]": coco_eval.stats[4],
        "AP@[large]": coco_eval.stats[5],
        "AR@[maxDets=1]": coco_eval.stats[6],
        "AR@[maxDets=10]": coco_eval.stats[7],
        "AR@[maxDets=100]": coco_eval.stats[8],
        "AR@[small]": coco_eval.stats[9],
        "AR@[medium]": coco_eval.stats[10],
        "AR@[large]": coco_eval.stats[11],
    }
    return metrics
